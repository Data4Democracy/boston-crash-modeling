{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "os.chdir('../src/data')\n",
    "import util\n",
    "os.chdir('../../notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Right': 563, u'crash_count': 1, u'Longitude': u'-71.0631259', u'Filename': u'7436_2407_BUNKER-HILL-ST,-MYSTIC-ST,-SCHOOL-ST_NA_NA_CHARLESTOWN_11-HOURS_NA_09-18-2014.XLS', u'Hours': u'11', u'Date': u'2014-09-18', u'near_intersection_id': u'8032', u'Normalized': 6046, u'Address': u'Mystic St & Bunker Hill St, Boston, MA 02129, USA', u'Latitude': u'42.379195', u'near_id': 8032, u'Total': 4430, u'Conflict': 529, u'Left': 529}\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_FP = '../data/processed/'\n",
    "\n",
    "inters = {}\n",
    "with open(PROCESSED_DATA_FP + 'inters_data.json') as f:\n",
    "    data = json.load(f)\n",
    "    for key, value in data.iteritems():\n",
    "        inters[str(key)] = value[0]\n",
    "\n",
    "with open(PROCESSED_DATA_FP + 'tmc_summary.json') as f:\n",
    "    addresses = json.load(f)\n",
    "\n",
    "    print addresses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_crashes():\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    inters = {}\n",
    "    with open(PROCESSED_DATA_FP + 'inters_data.json') as f:\n",
    "        data = json.load(f)\n",
    "        for key, value in data.iteritems():\n",
    "            inters[str(key)] = value[0]\n",
    "\n",
    "    \n",
    "    crashes_by_seg = {}\n",
    "    with open(PROCESSED_DATA_FP + 'crash_joined.json') as f:\n",
    "        data = json.load(f)\n",
    "        for row in data:\n",
    "            if str(row['near_id']) == '':\n",
    "                next\n",
    "            if str(row['near_id']) not in crashes_by_seg.keys():\n",
    "                crashes_by_seg[str(row['near_id'])] = {\n",
    "                    'total': 0, 'type': [], 'values': []}\n",
    "            crashes_by_seg[str(row['near_id'])]['total'] += 1\n",
    "            crashes_by_seg[str(row['near_id'])]['type'].append(\n",
    "                row['FIRST_EVENT_SUBTYPE'])\n",
    "            crashes_by_seg[str(row['near_id'])]['values'].append(row)\n",
    "            \n",
    "    crash_items, crash_locations = util.group_json_by_location(PROCESSED_DATA_FP + 'crash_joined.json')\n",
    "\n",
    "    ccount = 0\n",
    "    tcount = 0\n",
    "   \n",
    "    counts = {\n",
    "        'low_volume': 0,\n",
    "        'high_volume': 0,\n",
    "        'low_volume_crash': 0,\n",
    "        'high_volume_crash': 0,\n",
    "        'low_speed': 0,\n",
    "        'low_speed_crash': 0,\n",
    "        'high_speed': 0,\n",
    "        'high_speed_crash': 0,\n",
    "        'low_conflict': 0,\n",
    "        'low_conflict_crash': 0,\n",
    "        'high_conflict': 0,\n",
    "        'high_conflict_crash': 0,\n",
    "        'high_left': 0,\n",
    "        'high_left_crash': 0,\n",
    "        'high_right': 0,\n",
    "        'high_right_crash': 0,\n",
    "    }\n",
    "    with open(PROCESSED_DATA_FP + 'tmc_summary.json') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        for row in data:\n",
    "            if row['near_intersection_id']:\n",
    "                inter_info = inters[row['near_intersection_id']]\n",
    "                speed = inter_info['SPEEDLIMIT']\n",
    "                volume = row['Normalized']\n",
    "                \n",
    "                # speed binning\n",
    "                if speed <= 25:\n",
    "                    counts['low_speed'] += 1\n",
    "                else:\n",
    "                    counts['high_speed'] += 1\n",
    "                \n",
    "                # volume binning\n",
    "                if volume < 20000:\n",
    "                    counts['low_volume'] += 1\n",
    "                else:\n",
    "                    counts['high_volume'] += 1\n",
    "\n",
    "                # conflict binning\n",
    "                if row['Conflict'] < 1000:\n",
    "                    counts['low_conflict'] += 1\n",
    "                else:\n",
    "                    counts['high_conflict'] += 1\n",
    "\n",
    "                if row['Left'] > 1000:\n",
    "                    counts['high_left'] += 1\n",
    "                if row['Right'] > 1000:\n",
    "                    counts['high_right'] += 1\n",
    "\n",
    "                if row['near_intersection_id'] in crash_locations.keys():\n",
    "                    crash_count = crash_locations[row['near_intersection_id']]['count']\n",
    "                    if crash_count:\n",
    "                        ccount += 1\n",
    "\n",
    "                    if speed <= 25:\n",
    "                        counts['low_speed_crash'] += 1\n",
    "                    else:\n",
    "                        counts['high_speed_crash'] += 1\n",
    "                \n",
    "                    # volume binning\n",
    "                    if volume < 20000:\n",
    "                        counts['low_volume_crash'] += 1\n",
    "                    else:\n",
    "                        counts['high_volume_crash'] += 1\n",
    "\n",
    "                    if row['Conflict'] < 1000:\n",
    "                        counts['low_conflict_crash'] += 1\n",
    "                    else:\n",
    "                        counts['high_conflict_crash'] += 1\n",
    "\n",
    "                    if row['Left'] > 1000:\n",
    "                        counts['high_left_crash'] += 1\n",
    "                    if row['Right'] > 1000:\n",
    "                        counts['high_right_crash'] += 1\n",
    "\n",
    "                tcount += 1\n",
    "\n",
    "    print 'Percentage of locations with crash:' + str(round(100 * ccount/tcount)) + ' out of ' + str(tcount)\n",
    "    print 'Percentage of high volume with crash:' + str(round(100 * counts['high_volume_crash']/counts['high_volume'])) + ' out of ' + str(counts['high_volume'])\n",
    "    print 'Percentage of high speed limit with crash:' + str(round(100*counts['high_speed_crash']/counts['high_speed'])) + ' out of ' + str(counts['high_speed'])\n",
    "    print 'Percentage of high conflict with crash:' + str(round(100*counts['high_conflict_crash']/counts['high_conflict'])) + ' out of ' + str(counts['high_conflict'])\n",
    "    print 'Percentage of high left with crash:' + str(round(100*counts['high_left_crash']/counts['high_left'])) + ' out of ' + str(counts['high_left'])\n",
    "    print 'Percentage of high right with crash:' + str(round(100*counts['high_right_crash']/counts['high_right'])) + ' out of ' + str(counts['high_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tmcs(addresses):\n",
    "\n",
    "    # First create basemap                                                                                                                                              \n",
    "    points = folium.Map(\n",
    "        [42.3601, -71.0589],\n",
    "        tiles='Cartodb Positron',\n",
    "        zoom_start=12\n",
    "    )\n",
    "\n",
    "    # plot tmcs                                                                                                                                                         \n",
    "    for address in addresses:\n",
    "        if not pd.isnull(address['Latitude']):\n",
    "            folium.CircleMarker(\n",
    "                location=[float(address['Latitude']), float(address['Longitude'])],\n",
    "                fill_color='yellow', fill=True, fill_opacity=.7, color='yellow',\n",
    "                radius=6.0).add_to(points)\n",
    "\n",
    "    # Plot atrs                                                                                                                                                         \n",
    "    atrs = util.csv_to_projected_records(\n",
    "        PROCESSED_DATA_FP + 'geocoded_atrs.csv', x='lng', y='lat')\n",
    "    for atr in atrs:\n",
    "        properties = atr['properties']\n",
    "        if properties['lat']:\n",
    "            folium.CircleMarker(\n",
    "                location=[float(properties['lat']), float(properties['lng'])],\n",
    "                fill_color='green', fill=True, fill_opacity=.7, color='grey',\n",
    "                radius=6.0).add_to(points)\n",
    "\n",
    "    display(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of locations with crash:62.0 out of 335\n",
      "Percentage of high volume with crash:83.0 out of 100\n",
      "Percentage of high speed limit with crash:70.0 out of 86\n",
      "Percentage of high conflict with crash:75.0 out of 168\n",
      "Percentage of high left with crash:75.0 out of 176\n",
      "Percentage of high right with crash:71.0 out of 173\n"
     ]
    }
   ],
   "source": [
    "compare_crashes()\n",
    "#plot_tmcs(addresses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
